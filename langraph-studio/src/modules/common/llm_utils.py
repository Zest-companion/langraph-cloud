"""
Utilitaires pour les appels LLM isol√©s
"""
import logging
import unicodedata
from typing import Dict, List
from .config import analysis_llm

logger = logging.getLogger(__name__)

def isolated_analysis_call_with_messages(system_content: str, user_content: str, model: str = "gpt-3.5-turbo") -> str:
    """
    Appel OpenAI direct avec messages syst√®me et utilisateur s√©par√©s
    Garantit l'isolation compl√®te du streaming LangGraph
    """
    try:
        import openai
        import os
        
        # Cr√©er un client compl√®tement isol√©
        isolated_client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            timeout=30.0
        )
        
        logger.info("üîí Using completely isolated OpenAI client with separate messages...")
        
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ]
        
        response = isolated_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.7,  # Augment√© de 0.3 √† 0.7 pour plus de flexibilit√©
            max_tokens=500,
            stream=False,  # Absolument pas de streaming
            logprobs=False,  # D√©sactiver les logprobs
        )
        
        result = response.choices[0].message.content.strip()
        logger.info(f"üîí Isolated call completed: {len(result)} chars")
        return result
        
    except ImportError:
        logger.warning("‚ö†Ô∏è OpenAI module not available, using fallback")
        return fallback_analysis_call_with_messages(system_content, user_content)
    except Exception as e:
        logger.error(f"‚ùå Isolated analysis call failed: {e}")
        return fallback_analysis_call_with_messages(system_content, user_content)

def fallback_analysis_call_with_messages(system_content: str, user_content: str) -> str:
    """Fallback using LangChain analysis_llm with separate messages"""
    try:
        from langchain_core.messages import HumanMessage, SystemMessage
        logger.info("üîÑ Using LangChain fallback with separate messages...")
        messages = [
            SystemMessage(content=system_content),
            HumanMessage(content=user_content)
        ]
        response = analysis_llm.invoke(messages)
        return response.content
    except Exception as e:
        logger.error(f"‚ùå Fallback analysis also failed: {e}")
        return '{"question_type": "PERSONAL_DEVELOPMENT", "instructions": "CALL_AB: Tool A + Tool B", "other_mbti_profiles": null, "continuity_detected": false}'

def isolated_analysis_call(prompt_content: str) -> str:
    """
    Appel OpenAI direct sans passer par LangChain
    Garantit l'isolation compl√®te du streaming LangGraph
    """
    try:
        import openai
        import os
        
        # Cr√©er un client compl√®tement isol√©
        isolated_client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            timeout=30.0
        )
        
        logger.info("üîí Using completely isolated OpenAI client...")
        
        # S√©parer le prompt syst√®me du contenu utilisateur si possible
        if "\n\n" in prompt_content and prompt_content.startswith("You are"):
            # D√©tection d'un prompt compos√© (system + user content)
            parts = prompt_content.split("\n\nOriginal User Input:", 1)
            if len(parts) == 2:
                system_part = parts[0]
                user_part = "Original User Input:" + parts[1]
                messages = [
                    {"role": "system", "content": system_part},
                    {"role": "user", "content": user_part}
                ]
            else:
                messages = [{"role": "user", "content": prompt_content}]
        else:
            messages = [{"role": "user", "content": prompt_content}]
        
        response = isolated_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.7,  # Augment√© de 0.3 √† 0.7 pour plus de flexibilit√©
            max_tokens=500,
            stream=False,  # Absolument pas de streaming
            logprobs=False,  # D√©sactiver les logprobs
        )
        
        result = response.choices[0].message.content.strip()
        logger.info(f"üîí Isolated call completed: {len(result)} chars")
        return result
        
    except ImportError:
        logger.warning("‚ö†Ô∏è OpenAI module not available, using fallback")
        return fallback_analysis_call(prompt_content)
    except Exception as e:
        logger.error(f"‚ùå Isolated analysis call failed: {e}")
        return fallback_analysis_call(prompt_content)

def fallback_analysis_call(prompt_content: str) -> str:
    """Fallback using LangChain analysis_llm"""
    try:
        from langchain_core.messages import HumanMessage
        logger.info("üîÑ Using LangChain fallback for analysis...")
        response = analysis_llm.invoke([HumanMessage(content=prompt_content)])
        return response.content
    except Exception as e:
        logger.error(f"‚ùå Fallback analysis also failed: {e}")
        return '{"question_type": "PERSONAL_DEVELOPMENT", "instructions": "CALL_AB: Tool A + Tool B", "other_mbti_profiles": null, "continuity_detected": false}'

# Fonction pour g√©rer les messages manuellement sans auto-ajout
def normalize_name_for_metadata(name: str) -> str:
    """
    Normalise un nom pour matcher le format utilis√© dans les m√©tadonn√©es de vectorisation.
    jean-pierre_aerts -> jean_pierre_aerts (remplace tirets par underscores)
    """
    if not name:
        return name
    # Remplacer les tirets par des underscores pour matcher les m√©tadonn√©es
    normalized = name.replace('-', '_')
    return normalized

def manage_messages(existing_messages, new_messages):
    """G√©rer manuellement les messages sans ajouter automatiquement toutes les r√©ponses LLM"""
    if not existing_messages:
        existing_messages = []
    
    # Seulement ajouter les messages utilisateur et les r√©ponses finales explicites
    result = existing_messages[:]
    for msg in new_messages:
        if msg.get('type') == 'user' or msg.get('role') == 'user':
            result.append(msg)
        elif msg.get('type') == 'assistant' or msg.get('role') == 'assistant':
            # Seulement ajouter si c'est explicitement marqu√© comme r√©ponse finale
            if msg.get('is_final_response', False):
                result.append(msg)
    
    return result