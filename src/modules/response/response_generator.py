"""
G√©n√©rateur de r√©ponses finales pour les diff√©rents sous-th√®mes
"""
import logging
from typing import Dict, List, Optional
from langchain_core.messages import SystemMessage, HumanMessage
from ..common.types import WorkflowState
from ..common.config import llm
from ..common.llm_utils import manage_messages
from ..prompts.prompt_builder import build_lencioni_prompt, build_mbti_prompt, create_prompt_by_subtheme

logger = logging.getLogger(__name__)

def generate_final_response(state: WorkflowState) -> WorkflowState:
    """G√©n√©rateur unifi√© pour tous les sous-th√®mes avec streaming garanti"""
    
    # D√©terminer le sous-th√®me pour choisir le bon template
    sub_theme = state.get('sub_theme', '')
    theme = state.get('theme', '')
    
    logger.info(f"ü§ñ NODE 6: Generating response for theme={theme}, sub_theme={sub_theme}")
    
    try:
        # R√©cup√©rer la question utilisateur
        user_question = state.get('user_message', '') or (state.get('messages', [{}])[-1].get('content', '') if state.get('messages') else '')
        
        # ============= SYST√àME DE TEMPLATES UNIFI√â =============
        # Utiliser le factory central pour tous les sous-th√®mes
        system_prompt = create_prompt_by_subtheme(sub_theme, state)
        
        # Debug: Show what context is being sent to the final agent
        logger.info(f"üîç CONTEXT SENT TO FINAL AGENT:")
        logger.info(f"  - Tool A results: {len(state.get('personalized_content', []))} items")
        logger.info(f"  - Tool B results: {len(state.get('generic_content', []))} items") 
        logger.info(f"  - Tool C results: {len(state.get('others_content', []))} items")
        logger.info(f"  - Tool D results: {len(state.get('general_content', []))} items")
        logger.info(f"  - General results: {len(state.get('general_vector_results', []))} items")
        logger.info(f"  - Lencioni results: {len(state.get('lencioni_vector_results', []))} items")
        logger.info(f"  - System prompt length: {len(system_prompt)} characters")
        
        # Debug: Show complete system prompt for LangGraph Studio  
        logger.info(f"üìÑ COMPLETE SYSTEM PROMPT SENT TO LLM:")
        logger.info(f"{'='*60}")
        logger.info(system_prompt)
        logger.info(f"{'='*60}")
        logger.info(f"üìù USER MESSAGE:")
        logger.info(f"{state.get('user_message', 'No user message')}")
        
        # Messages pour le LLM
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_question)
        ]
        
        # ============= STREAMING UNIFI√â POUR TOUS LES TEMPLATES =============
        logger.info(f"üîÑ Starting {sub_theme} response streaming...")
        final_response = ""
        
        try:
            # Utiliser streaming explicite pour que LangGraph puisse capturer les tokens
            final_response = ""
            for chunk in llm.stream(messages):
                if chunk.content:
                    final_response += chunk.content
                    # Les tokens sont automatiquement captur√©s par LangGraph Studio
            
            logger.info(f"‚úÖ {sub_theme} response generated via streaming ({len(final_response)} chars)")
            
            # Pr√©parer le message final
            final_assistant_message = {
                "role": "assistant",
                "content": final_response,
                "type": "assistant",
                "is_final_response": True
            }
            
            # G√©rer les messages
            updated_messages = manage_messages(state.get('messages', []), [final_assistant_message])
            
            return {
                "final_response": final_response,
                "messages": updated_messages,
                "user_id": state.get("user_id"),
                "user_name": state.get("user_name"),
                "user_email": state.get("user_email"),
                "user_mbti": state.get("user_mbti"),
                "lencioni_data": state.get("lencioni_data"),
                # system_prompt_debug gard√© pour LangGraph Studio seulement
                "system_prompt_debug": system_prompt
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error generating {sub_theme} response: {e}")
            return {
                "final_response": f"D√©sol√©, une erreur s'est produite lors de l'analyse. Pouvez-vous reformuler votre question ?",
                "messages": state.get('messages', []),
                "user_id": state.get("user_id"),
                "user_name": state.get("user_name"),
                "user_email": state.get("user_email"),
                "user_mbti": state.get("user_mbti")
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error in generate_final_response: {e}")
        return {
            "final_response": "D√©sol√©, une erreur s'est produite. Pouvez-vous reformuler votre question ?",
            "messages": state.get('messages', []),
            "user_id": state.get("user_id"),
            "user_name": state.get("user_name"),
            "user_email": state.get("user_email"),
            "user_mbti": state.get("user_mbti")
        }


def generate_final_response_original(state: WorkflowState) -> WorkflowState:
    """Version originale conserv√©e pour r√©f√©rence - reproduit l'√©tape 6 du workflow n8n"""
    logger.info("ü§ñ NODE 6: Generating final response (ORIGINAL)...")
    
    try:
        # R√©cup√©rer la question utilisateur
        user_question = state.get('user_message', '') or (state.get('messages', [{}])[-1].get('content', '') if state.get('messages') else '')
        
        # Ajouter l'historique de conversation pour √©viter les r√©p√©titions
        conversation_history = []
        for msg in state.get('messages', [])[-3:]:  # 3 derniers messages pour contexte
            if hasattr(msg, 'type') and hasattr(msg, 'content'):
                role = "User" if msg.type == 'human' else "Assistant"
                conversation_history.append(f"{role}: {msg.content}")
            elif isinstance(msg, dict):
                role = "User" if msg.get('role') == 'user' else "Assistant"
                conversation_history.append(f"{role}: {msg.get('content', '')}")
        
        history_text = "\n".join(conversation_history) if conversation_history else "No previous conversation"
        
        # Construire le prompt syst√®me avec tous les contextes
        system_prompt = f""" 

ROLE: 
You are ZEST COMPANION, an expert leadership mentor & MBTI coach. You coach with concise yet comprehensive answers, prioritizing specificity and actionable guidance to help the user become a more impactful leader. 

GUARDRAILS: 
- Use ONLY the temperament description and ZEST database search results provided. 
- Do not use MBTI nicknames or external MBTI knowledge unless in context. 
- SCOPE: Focus on MBTI leadership coaching, team dynamics, and professional development only
- REDIRECT: For mental health, family issues, clinical conditions, or therapy needs - recommend consulting appropriate professional resources
- If no relevant info, recommend contacting jean-pierre.aerts@zestforleaders.com.


USER QUESTION: "{user_question}"

RECENT CONVERSATION HISTORY:
{history_text}

User MBTI Profile: {state.get('user_mbti', 'Unknown')}
User Temperament: {state.get('user_temperament', 'Unknown')}
Temperament Description: {state.get('temperament_description', 'Unknown')}
"""
        
        # Ajouter le contenu personnalis√©, g√©n√©rique, autres profils et g√©n√©ral
        personalized_content = state.get('personalized_content', [])
        generic_content = state.get('generic_content', [])
        others_content = state.get('others_content', [])
        general_content = state.get('general_content', [])
        temperament_content = state.get('temperament_content', [])
        
        if personalized_content:
            system_prompt += "\n\nPERSONALIZED CONTENT (user's specific profile):\n"
            for item in personalized_content[:3]:  # Limiter √† 3 items pour √©viter trop de contexte
                system_prompt += f"- {item.get('content', '')[:500]}...\n"
                
        if generic_content:
            system_prompt += "\n\nGENERIC USER TYPE CONTENT:\n"
            for item in generic_content[:3]:
                system_prompt += f"- {item.get('content', '')[:500]}...\n"
                
        if others_content:
            system_prompt += "\n\nOTHER MBTI TYPES CONTENT:\n"
            for item in others_content[:3]:
                target_mbti = item.get('metadata', {}).get('target_mbti', 'Unknown')
                system_prompt += f"- {target_mbti}: {item.get('content', '')[:500]}...\n"
                
        if general_content:
            system_prompt += "\n\nGENERAL MBTI CONTENT:\n"
            for item in general_content[:3]:
                system_prompt += f"- {item.get('content', '')[:500]}...\n"
                
        if temperament_content:
            system_prompt += "\n\nTEMPERAMENT CONTENT:\n"
            for item in temperament_content[:2]:  # Limiter davantage pour les temp√©raments
                temperament_info = item.get('temperament_info', 'Unknown')
                target = item.get('target', 'Unknown')
                system_prompt += f"- {temperament_info} (for {target}): {item.get('content', '')[:400]}...\n"
        
        system_prompt += """

Your task is to provide personalized, actionable guidance based on the user's MBTI profile and the search results provided above. 
Be conversational, supportive, and specific in your advice. Use the content to inform your response but make it engaging and tailored to their question.
"""
        
        # Debug: Show what context is being sent to the final agent
        logger.info(f"üîç ORIGINAL CONTEXT SENT TO FINAL AGENT:")
        logger.info(f"  - Personalized: {len(personalized_content)} items")
        logger.info(f"  - Generic: {len(generic_content)} items") 
        logger.info(f"  - Others: {len(others_content)} items")
        logger.info(f"  - General: {len(general_content)} items")
        logger.info(f"  - Temperament: {len(temperament_content)} items")
        logger.info(f"  - System prompt length: {len(system_prompt)} characters")
        
        # Messages pour le LLM
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_question)
        ]
        
        # Streaming pour la r√©ponse finale
        logger.info("üîÑ Starting original response streaming...")
        final_response = ""
        
        try:
            for chunk in llm.stream(messages):
                if chunk.content:
                    final_response += chunk.content
            
            logger.info(f"‚úÖ Original response generated via streaming ({len(final_response)} chars)")
            
            # Pr√©parer le message final
            final_assistant_message = {
                "role": "assistant", 
                "content": final_response,
                "type": "assistant",
                "is_final_response": True
            }
            
            # G√©rer les messages
            updated_messages = manage_messages(state.get('messages', []), [final_assistant_message])
            
            return {
                "final_response": final_response,
                "messages": updated_messages,
                "user_id": state.get("user_id"),
                "user_name": state.get("user_name"),
                "user_email": state.get("user_email"),
                "user_mbti": state.get("user_mbti"),
                "system_prompt_debug": system_prompt
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in original response streaming: {e}")
            return {
                "final_response": "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse.",
                "messages": state.get('messages', []),
                "user_id": state.get("user_id"),
                "user_name": state.get("user_name"),
                "user_email": state.get("user_email"),
                "user_mbti": state.get("user_mbti")
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error in generate_final_response_original: {e}")
        return {
            "final_response": "D√©sol√©, une erreur s'est produite.",
            "messages": state.get('messages', []),
            "user_id": state.get("user_id"),
            "user_name": state.get("user_name"),
            "user_email": state.get("user_email"),
            "user_mbti": state.get("user_mbti")
        }


def create_error_response(error_message: str, state: WorkflowState) -> WorkflowState:
    """Cr√©e une r√©ponse d'erreur standardis√©e"""
    return {
        "final_response": f"D√©sol√©, {error_message}. Pouvez-vous reformuler votre question ?",
        "messages": state.get('messages', []),
        "user_id": state.get("user_id"),
        "user_name": state.get("user_name"),  
        "user_email": state.get("user_email"),
        "user_mbti": state.get("user_mbti")
    }


def format_response_context(state: WorkflowState) -> Dict:
    """Formate le contexte pour la g√©n√©ration de r√©ponse"""
    return {
        "personalized_count": len(state.get('personalized_content', [])),
        "generic_count": len(state.get('generic_content', [])),
        "others_count": len(state.get('others_content', [])),
        "general_count": len(state.get('general_content', [])),
        "temperament_count": len(state.get('temperament_content', [])),
        "lencioni_count": len(state.get('lencioni_data', [])),
        "has_user_data": bool(state.get('user_mbti') and state.get('user_mbti') != 'Unknown')
    }